{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11850506,"sourceType":"datasetVersion","datasetId":7446208}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport os\nimport wandb\nfrom tqdm import tqdm\nimport re\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:05.444463Z","iopub.execute_input":"2025-05-20T19:20:05.445009Z","iopub.status.idle":"2025-05-20T19:20:12.680521Z","shell.execute_reply.started":"2025-05-20T19:20:05.444987Z","shell.execute_reply":"2025-05-20T19:20:12.679901Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\n\nos.environ['WANDB_API_KEY'] = secret_value_0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:12.681806Z","iopub.execute_input":"2025-05-20T19:20:12.682272Z","iopub.status.idle":"2025-05-20T19:20:12.947879Z","shell.execute_reply.started":"2025-05-20T19:20:12.682247Z","shell.execute_reply":"2025-05-20T19:20:12.947320Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"DEVICE   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBASE_DIR = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/te/lexicons' # Change 'te' → desired language","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:12.948679Z","iopub.execute_input":"2025-05-20T19:20:12.948862Z","iopub.status.idle":"2025-05-20T19:20:13.009790Z","shell.execute_reply.started":"2025-05-20T19:20:12.948847Z","shell.execute_reply":"2025-05-20T19:20:13.008991Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class CharacterEmbedding(nn.Module):\n    # Creating an embedding layer that maps input character indices to embedding vectors.\n    # input_size: number of unique characters (vocabulary size)\n    # embedding_dim: size of each embedding vector\n    def __init__(self, input_size, embedding_dim):\n        super(CharacterEmbedding, self).__init__()\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n\n    # Returns corresponding embedding vectors of shape (batch_size, seq_length, embedding_dim)\n    def forward(self, input_seq):\n        # input_seq: a tensor of character indices, typically of shape (batch_size, seq_length)\n        return self.embedding(input_seq)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:13.011553Z","iopub.execute_input":"2025-05-20T19:20:13.011788Z","iopub.status.idle":"2025-05-20T19:20:13.027671Z","shell.execute_reply.started":"2025-05-20T19:20:13.011769Z","shell.execute_reply":"2025-05-20T19:20:13.026703Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# EncoderRNN transforms sequences of token IDs into contextual hidden states\n# Supports GRU, LSTM, or vanilla RNN cells\n# input_size: number of unique tokens\n# hidden_size: size of the RNN hidden state\n# embedding_dim: size of token embedding vectors\n# num_layers: number of stacked recurrent layers\n# cell_type: 'GRU', 'LSTM', or 'RNN'\n# dropout_p: dropout probability between RNN layers (only if num_layers > 1)\n# bidirectional: whether to run the RNN in both forward and backward directions\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, embedding_dim, num_layers=1,cell_type='GRU', dropout_p=0.1, bidirectional=False):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional\n        self.directions = 2 if bidirectional else 1\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        \n        # Dropout before the RNN (applied to embeddings)\n        self.dropout = nn.Dropout(dropout_p)\n        dropout_p = dropout_p if num_layers > 1 else 0\n        \n        # RNN layer\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, batch_first=True)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, batch_first=True)\n        else:  # Default to RNN\n            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, nonlinearity='tanh', batch_first=True)\n\n    # Forward pass through the encoder\n    def forward(self, input_seq):\n        # Input shape: [batch_size, seq_len]\n        batch_size = input_seq.size(0)\n        \n        # Convert indices to embeddings and apply dropout to embeddings\n        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embedding_dim]\n        embedded = self.dropout(embedded)\n        \n        # Pass through RNN\n        outputs, hidden = self.rnn(embedded)\n        \n        return outputs, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:13.028366Z","iopub.execute_input":"2025-05-20T19:20:13.028635Z","iopub.status.idle":"2025-05-20T19:20:13.039989Z","shell.execute_reply.started":"2025-05-20T19:20:13.028611Z","shell.execute_reply":"2025-05-20T19:20:13.039250Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.hidden_size = hidden_size\n        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n        self.v = nn.Parameter(torch.rand(hidden_size))\n        nn.init.uniform_(self.v, -0.1, 0.1)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden: [batch_size, hidden_size]\n        # encoder_outputs: [batch_size, seq_len, hidden_size]\n\n        batch_size = encoder_outputs.size(0)\n        seq_len = encoder_outputs.size(1)\n\n        # Repeat hidden state seq_len times to concat with encoder outputs\n        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # [batch_size, seq_len, hidden_size]\n\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, seq_len, hidden_size]\n        energy = energy.transpose(1, 2)  # [batch_size, hidden_size, seq_len]\n\n        v = self.v.repeat(batch_size, 1).unsqueeze(1)  # [batch_size, 1, hidden_size]\n\n        energy = torch.bmm(v, energy).squeeze(1)  # [batch_size, seq_len]\n\n        attn_weights = F.softmax(energy, dim=1)  # [batch_size, seq_len]\n\n        return attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:13.040858Z","iopub.execute_input":"2025-05-20T19:20:13.041114Z","iopub.status.idle":"2025-05-20T19:20:13.058444Z","shell.execute_reply.started":"2025-05-20T19:20:13.041088Z","shell.execute_reply":"2025-05-20T19:20:13.057651Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class DecoderRNNWithAttention(nn.Module):\n    def __init__(self, output_size, hidden_size, embedding_dim, num_layers=1, \n                 cell_type='GRU', dropout_p=0.1):\n        \n        super(DecoderRNNWithAttention, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        \n        # Embedding layer for target characters\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        \n        # Dropout applied before RNN\n        self.dropout = nn.Dropout(dropout_p)\n\n        dropout_p = dropout_p if num_layers > 1 else 0\n        \n        # RNN input size is embedding_dim + hidden_size (due to attention context)\n        rnn_input_size = embedding_dim + hidden_size\n        \n        # RNN layer\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(rnn_input_size, hidden_size, num_layers, dropout=dropout_p, batch_first=True)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(rnn_input_size, hidden_size, num_layers, dropout=dropout_p, batch_first=True)\n        else:\n            self.rnn = nn.RNN(rnn_input_size, hidden_size, num_layers, dropout=dropout_p, nonlinearity='tanh', batch_first=True)\n            \n        # Attention mechanism\n        self.attention = Attention(hidden_size)\n        \n        # Output layer with dropout\n        self.out_dropout = nn.Dropout(dropout_p)\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_char, hidden, encoder_outputs):\n        # input_char: [batch_size] (current input token indices)\n        # hidden: (h_n, c_n) for LSTM or h_n for GRU/RNN\n        # encoder_outputs: [batch_size, seq_len, hidden_size]\n        \n        batch_size = input_char.size(0)\n        \n        # Embed input character and apply dropout\n        embedded = self.embedding(input_char.squeeze(1)).unsqueeze(1)  # [batch_size, 1, embedding_dim]\n        embedded = self.dropout(embedded)\n        \n        # Get the last hidden state from hidden (handle LSTM tuple)\n        if self.cell_type == 'LSTM':\n            last_hidden = hidden[0][-1]  # [batch_size, hidden_size]\n        else:\n            last_hidden = hidden[-1]     # [batch_size, hidden_size]\n        \n        # Calculate attention weights and context vector\n        attn_weights = self.attention(last_hidden, encoder_outputs)  # [batch_size, seq_len]\n        \n        attn_weights = attn_weights.unsqueeze(1)  # [batch_size, 1, seq_len]\n        \n        # Compute context vector as weighted sum of encoder outputs\n        context = torch.bmm(attn_weights, encoder_outputs)  # [batch_size, 1, hidden_size]\n        \n        # Concatenate embedded input and context vector\n        \n        rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, embedding_dim + hidden_size]\n        \n        # Pass through RNN\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        # Output shape: [batch_size, 1, hidden_size]\n        output = self.out_dropout(output)\n        output = self.out(output.squeeze(1))  # [batch_size, output_size]\n        \n        return F.log_softmax(output, dim=1), hidden, attn_weights.squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:13.059166Z","iopub.execute_input":"2025-05-20T19:20:13.059366Z","iopub.status.idle":"2025-05-20T19:20:13.073247Z","shell.execute_reply.started":"2025-05-20T19:20:13.059349Z","shell.execute_reply":"2025-05-20T19:20:13.072619Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def beam_search_decode(model, src, sos_idx, eos_idx, max_len=30, beam_width=3, device='cuda'):\n    model.eval()\n    with torch.no_grad():\n        # Encode input\n        encoder_outputs, encoder_hidden = model.encoder(src)\n\n        # Prepare initial decoder hidden state\n        if model.bidirectional:\n            if model.cell_type == 'LSTM':\n                h_n, c_n = encoder_hidden\n                h_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size, device=device)\n                c_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size, device=device)\n                for layer in range(model.encoder.num_layers):\n                    h_combined = torch.cat((h_n[2*layer], h_n[2*layer+1]), dim=1)\n                    c_combined = torch.cat((c_n[2*layer], c_n[2*layer+1]), dim=1)\n                    h_dec[layer] = model.hidden_transform(h_combined)\n                    c_dec[layer] = model.hidden_transform(c_combined)\n                decoder_hidden = (h_dec, c_dec)\n            else:\n                decoder_hidden = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size, device=device)\n                for layer in range(model.encoder.num_layers):\n                    h_combined = torch.cat((encoder_hidden[2*layer], encoder_hidden[2*layer+1]), dim=1)\n                    decoder_hidden[layer] = model.hidden_transform(h_combined)\n        else:\n            decoder_hidden = encoder_hidden\n\n        # Beam search initialization\n        beams = [([sos_idx], 0.0, decoder_hidden)]  # (sequence, cumulative log-prob, hidden)\n        completed = []\n\n        for _ in range(max_len):\n            new_beams = []\n            for seq, score, hidden in beams:\n                if seq[-1] == eos_idx:\n                    completed.append((seq, score))\n                    continue\n                input_char = torch.tensor([seq[-1]], device=device)\n\n                # Decoder forward with attention requires encoder_outputs\n                output, hidden_new, _attn = model.decoder(input_char, hidden, encoder_outputs)\n\n                log_probs = output  # Already log_softmax, shape [batch_size=1, output_size]\n                topk_log_probs, topk_indices = torch.topk(log_probs.squeeze(0), beam_width)\n                for k in range(beam_width):\n                    next_seq = seq + [topk_indices[k].item()]\n                    next_score = score + topk_log_probs[k].item()\n\n                    # Make sure to detach hidden state to avoid graph buildup\n                    if model.cell_type == 'LSTM':\n                        h_new = tuple(h.detach() for h in hidden_new)\n                        new_beams.append((next_seq, next_score, h_new))\n                    else:\n                        new_beams.append((next_seq, next_score, hidden_new.detach()))\n\n            # Keep top beam_width beams\n            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n            if not beams:\n                break\n\n        # Add any remaining beams ending with <eos>\n        completed += [(seq, score) for seq, score, _ in beams if seq[-1] == eos_idx]\n\n        # If none ended with <eos>, just take the best\n        if not completed:\n            completed = beams\n\n        # Sort by score\n        completed = sorted(completed, key=lambda x: x[1], reverse=True)\n        return completed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:13.984620Z","iopub.execute_input":"2025-05-20T19:20:13.985251Z","iopub.status.idle":"2025-05-20T19:20:13.995515Z","shell.execute_reply.started":"2025-05-20T19:20:13.985216Z","shell.execute_reply":"2025-05-20T19:20:13.994745Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Seq2Seq implements an Encoder and Decoder for end-to-end sequence-to-sequence modeling\n# input_size: size of source vocabulary\n# output_size: size of target vocabulary\n# embedding_dim: dimension of embeddings in both encoder and decoder\n# hidden_size: size of hidden states in encoder and decoder (must match for vanilla seq2seq)\n# encoder_layers / decoder_layers: number of stacked RNN layers\n# cell_type: 'GRU', 'LSTM', or 'RNN'\n# dropout_p: dropout probability for embeddings and RNN layers\n# bidirectional_encoder: if True, runs encoder bidirectionally and transforms hidden state\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, input_size, output_size, embedding_dim=256, hidden_size=256,\n                 encoder_layers=1, decoder_layers=1, cell_type='GRU', dropout_p=0.2,\n                 bidirectional_encoder=False):\n        super(Seq2Seq, self).__init__()\n        \n        self.encoder = EncoderRNN(input_size, hidden_size, embedding_dim,\n                                  num_layers=encoder_layers, cell_type=cell_type,\n                                  dropout_p=dropout_p, bidirectional=bidirectional_encoder)\n        \n        self.bidirectional = bidirectional_encoder\n        directions = 2 if bidirectional_encoder else 1\n        \n        if bidirectional_encoder:\n            self.hidden_transform = nn.Linear(hidden_size * directions, hidden_size)\n        \n        self.decoder = DecoderRNNWithAttention(output_size, hidden_size, embedding_dim,\n                                               num_layers=decoder_layers, cell_type=cell_type,\n                                               dropout_p=dropout_p)\n        \n        self.cell_type = cell_type\n\n    def _match_decoder_layers(self, hidden, batch_size):\n        if hidden.size(0) > self.decoder.num_layers:\n            return hidden[:self.decoder.num_layers]\n        elif hidden.size(0) < self.decoder.num_layers:\n            pad = torch.zeros(self.decoder.num_layers - hidden.size(0),\n                              batch_size, self.decoder.hidden_size,\n                              device=hidden.device)\n            return torch.cat([hidden, pad], dim=0)\n        else:\n            return hidden\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5, return_attention=False):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        output_size = self.decoder.output_size\n        \n        outputs = torch.zeros(batch_size, trg_len, output_size).to(src.device)\n        all_attentions = [] if return_attention else None\n\n        encoder_outputs, encoder_hidden = self.encoder(src)\n        decoder_hidden = None \n        \n        if self.bidirectional:\n            if self.cell_type == 'LSTM':\n                h_n, c_n = encoder_hidden\n                h_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(src.device)\n                c_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(src.device)\n                \n                for layer in range(self.decoder.num_layers):\n                    enc_layer = min(layer, self.encoder.num_layers - 1)\n                    h_combined = torch.cat((h_n[2 * enc_layer], h_n[2 * enc_layer + 1]), dim=1)\n                    c_combined = torch.cat((c_n[2 * enc_layer], c_n[2 * enc_layer + 1]), dim=1)\n                    h_dec[layer] = self.hidden_transform(h_combined)\n                    c_dec[layer] = self.hidden_transform(c_combined)\n                \n                decoder_hidden = (h_dec, c_dec)\n            \n            else:\n                h_n = encoder_hidden\n                h_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(src.device)\n                for layer in range(self.decoder.num_layers):\n                    enc_layer = min(layer, self.encoder.num_layers - 1)\n                    h_combined = torch.cat((h_n[2 * enc_layer], h_n[2 * enc_layer + 1]), dim=1)\n                    h_dec[layer] = self.hidden_transform(h_combined)\n                decoder_hidden = h_dec\n        \n        else:\n            if self.cell_type == \"LSTM\":\n                h, c = encoder_hidden\n                decoder_hidden = (\n                    self._match_decoder_layers(h, batch_size),\n                    self._match_decoder_layers(c, batch_size)\n                )\n            else:\n                decoder_hidden = self._match_decoder_layers(encoder_hidden, batch_size)\n        \n        input_char = trg[:, 0].unsqueeze(1)\n\n        for t in range(1, trg_len):\n            output, decoder_hidden, attn_weights = self.decoder(input_char, decoder_hidden, encoder_outputs)\n            outputs[:, t, :] = output\n\n            if return_attention:\n                all_attentions.append(attn_weights.unsqueeze(1))  # [batch, 1, src_len]\n\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1).unsqueeze(1)\n            input_char = trg[:, t].unsqueeze(1) if teacher_force else top1\n\n        if return_attention:\n            # Shape: [batch_size, trg_len-1, src_len]\n            all_attentions = torch.cat(all_attentions, dim=1)\n            return outputs, all_attentions\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:16.224970Z","iopub.execute_input":"2025-05-20T19:20:16.225249Z","iopub.status.idle":"2025-05-20T19:20:16.238552Z","shell.execute_reply.started":"2025-05-20T19:20:16.225232Z","shell.execute_reply":"2025-05-20T19:20:16.237961Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class LexiconDataset(Dataset):\n    def __init__(self, path, src_vocab=None, tgt_vocab=None, build_vocab=False):\n        self.pairs = []\n        with open(path, encoding='utf-8') as f:\n            for line in f:\n                cols = line.strip().split('\\t')\n                if len(cols) < 2:\n                    continue\n                tgt, src = cols[0], cols[1]  # Telugu is first column, romanized second\n                self.pairs.append((src, tgt)) # src = romanized, tgt = telugu\n\n        if build_vocab:\n            self.src_vocab = {'<pad>':0, '<sos>':1, '<eos>':2, '<unk>':3}\n            self.tgt_vocab = {'<pad>':0, '<sos>':1, '<eos>':2, '<unk>':3}\n            for rom, dev in self.pairs:\n                for c in rom:\n                    self.src_vocab.setdefault(c, len(self.src_vocab))\n                for c in dev:\n                    self.tgt_vocab.setdefault(c, len(self.tgt_vocab))\n        else:\n            assert src_vocab and tgt_vocab, \"Must provide vocabs if not building.\"\n            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        rom, dev = self.pairs[idx]\n        src_idxs = [self.src_vocab.get(c, self.src_vocab['<unk>']) for c in rom]\n        tgt_idxs = [self.tgt_vocab['<sos>']] + [self.tgt_vocab.get(c, self.tgt_vocab['<unk>']) for c in dev] + [self.tgt_vocab['<eos>']]\n        return torch.tensor(src_idxs, dtype=torch.long), torch.tensor(tgt_idxs, dtype=torch.long)\n\ndef collate_fn(batch):\n    \"\"\"\n    Pads all src/tgt sequences in the batch to the max length.\n    Returns:\n      padded_src: (batch_size, max_src_len)\n      padded_tgt: (batch_size, max_tgt_len)\n    \"\"\"\n    srcs, tgts = zip(*batch)\n    max_src = max(len(s) for s in srcs)\n    max_tgt = max(len(t) for t in tgts)\n\n    padded_src = torch.full((len(batch), max_src), 0, dtype=torch.long)\n    padded_tgt = torch.full((len(batch), max_tgt), 0, dtype=torch.long)\n    for i, (s, t) in enumerate(zip(srcs, tgts)):\n        padded_src[i, :len(s)] = s\n        padded_tgt[i, :len(t)] = t\n\n    return padded_src, padded_tgt\n\ndef get_dataloaders(base_dir, batch_size, build_vocab=False):\n    \"\"\"\n    Returns:\n      train_loader, val_loader, test_loader,\n      src_vocab_size, tgt_vocab_size, pad_index, src_vocab, tgt_vocab\n    \"\"\"\n    train_p = os.path.join(base_dir, 'te.translit.sampled.train.tsv')\n    dev_p   = os.path.join(base_dir, 'te.translit.sampled.dev.tsv')\n    test_p  = os.path.join(base_dir, 'te.translit.sampled.test.tsv')\n\n    train_ds = LexiconDataset(train_p, build_vocab=build_vocab)\n    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n    val_ds   = LexiconDataset(dev_p,  src_vocab, tgt_vocab)\n    test_ds  = LexiconDataset(test_p, src_vocab, tgt_vocab)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader  = DataLoader(test_ds,  batch_size=1,            shuffle=False, collate_fn=collate_fn)\n\n    return (train_loader, val_loader, test_loader,\n            len(src_vocab), len(tgt_vocab), src_vocab['<pad>'],\n            src_vocab, tgt_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:19.264743Z","iopub.execute_input":"2025-05-20T19:20:19.265016Z","iopub.status.idle":"2025-05-20T19:20:19.276659Z","shell.execute_reply.started":"2025-05-20T19:20:19.264994Z","shell.execute_reply":"2025-05-20T19:20:19.275887Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class EarlyStopper:\n    \"\"\"Stops a run if the monitored metric doesn’t improve for `patience` steps.\"\"\"\n    def __init__(self, patience=5, min_delta=1e-4):\n        self.patience, self.min_delta = patience, min_delta\n        self.counter, self.best = 0, None\n\n    def should_stop(self, current):\n        if self.best is None or current > self.best + self.min_delta:\n            self.best, self.counter = current, 0\n        else:\n            self.counter += 1\n        return self.counter >= self.patience","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:21.244426Z","iopub.execute_input":"2025-05-20T19:20:21.244669Z","iopub.status.idle":"2025-05-20T19:20:21.250085Z","shell.execute_reply.started":"2025-05-20T19:20:21.244653Z","shell.execute_reply":"2025-05-20T19:20:21.249173Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"CHAR2IDX_SRC = {\n    \"<pad>\": 0,\n    \"<sos>\": 1,\n    \"<eos>\": 2,\n    \"<unk>\": 3,\n    **{c: i + 4 for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n}\nIDX2CHAR_SRC = {i: c for c, i in CHAR2IDX_SRC.items()}\n\n# Load data, build vocabs, and create reverse target-char map\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\n\nIDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}  # Map decoder indices back to Telugu chars\n\n# Model, optimizer, loss, early stopping\n\nmodel = Seq2Seq(\n    input_size=src_size,\n    output_size=tgt_size,\n    embedding_dim=64,\n    hidden_size=128,\n    encoder_layers=3,\n    decoder_layers=3,\n    cell_type='GRU',  # or 'GRU' or 'RNN'\n    dropout_p=0.3,\n    bidirectional_encoder=False\n).to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.NLLLoss(ignore_index=pad_idx)\nstopper = EarlyStopper(patience=5)\nbest_val_acc = 0.0\n\n# Training Loop\n\nfor epoch in range(1, 11):\n    model.train()\n    total_loss = 0.0\n    for src, tgt in tqdm(train_loader, desc=f\"[Epoch {epoch}] Training\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        optimizer.zero_grad()\n        out = model(src, tgt, teacher_forcing_ratio=0.7)\n        loss = criterion(out.view(-1, tgt_size), tgt.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    # Validation: sequence-level accuracy\n    model.eval()\n    correct_seqs, total_seqs = 0, 0\n    with torch.no_grad():\n        for src, tgt in tqdm(val_loader, desc=f\"[Epoch {epoch}] Validation\", leave=False):\n            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n            out = model(src, tgt, teacher_forcing_ratio=0.0)\n            preds = out.argmax(dim=2)\n            for pred_seq, true_seq in zip(preds, tgt):\n                # Remove <sos> and padding tokens for comparison\n                pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                if torch.equal(pred_tokens, true_tokens):\n                    correct_seqs += 1\n                total_seqs += 1\n\n    val_acc = correct_seqs / total_seqs\n    print(f\"[Epoch {epoch}] Loss: {total_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # Optionally save model checkpoint here\n    elif stopper.should_stop(val_acc):\n        print(\"Early stopping triggered.\")\n        break\n\n# Final Test Evaluation\nmodel.eval()\ncorrect_seqs, total_seqs = 0, 0\nall_preds, all_trues = [], []\n\nwith torch.no_grad():\n    for src, tgt in tqdm(test_loader, desc=\"Final Test Eval\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out = model(src, tgt, teacher_forcing_ratio=0.0)\n        preds = out.argmax(dim=2)\n        for pred_seq, true_seq in zip(preds, tgt):\n            pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n            true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n            if torch.equal(pred_tokens, true_tokens):\n                correct_seqs += 1\n            total_seqs += 1\n\n            all_preds.append(pred_tokens)\n            all_trues.append(true_tokens)\n\ntest_acc = correct_seqs / total_seqs\nprint(f\"\\n Final Test Accuracy: {test_acc:.4f}\")\n\n\n# Sample Predictions in Telugu\nprint(\"\\n Sample Predictions:\")\nsample_indices = random.sample(range(len(all_preds)), min(10, len(all_preds)))\nfor idx in sample_indices:\n    pred_str = ''.join([IDX2CHAR_TGT[token.item()] for token in all_preds[idx]])\n    true_str = ''.join([IDX2CHAR_TGT[token.item()] for token in all_trues[idx]])\n    correctness = \"true\" if pred_str == true_str else \"false\"\n    print(f\"  True : {true_str}\\n  Pred : {pred_str}  {correctness}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:13:05.468625Z","iopub.execute_input":"2025-05-20T14:13:05.468890Z","iopub.status.idle":"2025-05-20T14:21:27.701294Z","shell.execute_reply.started":"2025-05-20T14:13:05.468862Z","shell.execute_reply":"2025-05-20T14:21:27.700474Z"}},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Loss: 1802.5624 | Val Acc: 0.0947\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Loss: 846.5610 | Val Acc: 0.2847\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Loss: 610.7603 | Val Acc: 0.3718\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Loss: 507.3234 | Val Acc: 0.4181\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Loss: 449.8098 | Val Acc: 0.4552\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Loss: 408.2083 | Val Acc: 0.4774\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Loss: 379.0115 | Val Acc: 0.4894\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Loss: 354.4138 | Val Acc: 0.5112\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Loss: 339.5995 | Val Acc: 0.5036\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Loss: 319.2981 | Val Acc: 0.5163\n","output_type":"stream"},{"name":"stderr","text":"                                                                     ","output_type":"stream"},{"name":"stdout","text":"\n Final Test Accuracy: 0.3598\n\n Sample Predictions:\n  True : తైత్తిరీయ<eos>\n  Pred : తైట్రీరీయా  false\n\n  True : లీలావతి<eos>\n  Pred : లేవతి<eos><eos><eos>  false\n\n  True : కలదీ<eos>\n  Pred : కలాది  false\n\n  True : జిగురు<eos>\n  Pred : జిగురు<eos>  true\n\n  True : అన్నింటినీ<eos>\n  Pred : అన్నింటిని<eos>  false\n\n  True : ఛ<eos>\n  Pred : <eos><eos>  false\n\n  True : రామాయణంలో<eos>\n  Pred : రామాయణంలో<eos>  true\n\n  True : స్వరములు<eos>\n  Pred : స్వరములు<eos>  true\n\n  True : వ్యవహరించే<eos>\n  Pred : వ్యవహరించే<eos>  true\n\n  True : దిగబడిన<eos>\n  Pred : దిగబడిన<eos>  true\n\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Define the sweep configuration\nsweep_config = {\n  'method': 'bayes',\n  'metric':     {'name': 'val_accuracy', 'goal': 'maximize'},\n  'early_terminate': {\n      'type': 'hyperband', 'min_iter': 2, 'max_iter': 8, 's': 2\n  },\n  'parameters': {\n    'embedding_dim': {'values': [16, 32, 64, 256]},\n    'hidden_size':    {'values': [16, 32, 64, 256]},\n    'encoder_layers': {'values': [1, 2, 3]},\n    'decoder_layers': {'values': [1, 2, 3]},\n    'cell_type':      {'values': ['RNN','GRU','LSTM']},\n    'dropout_p':      {'values': [0.2, 0.3, 0.4]},\n    'beam_width':     {'values': [1, 3, 5]},\n    'teacher_forcing_ratio' : {'values': [0.0, 0.3, 0.5,0.7,1.0]}\n  }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:26.199227Z","iopub.execute_input":"2025-05-20T19:20:26.199461Z","iopub.status.idle":"2025-05-20T19:20:26.204676Z","shell.execute_reply.started":"2025-05-20T19:20:26.199447Z","shell.execute_reply":"2025-05-20T19:20:26.203774Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train():\n    with wandb.init():\n        cfg = wandb.config\n        run_name = (\n            f\"emb{cfg.embedding_dim}_hid{cfg.hidden_size}\"\n            f\"_enc{cfg.encoder_layers}_dec{cfg.decoder_layers}\"\n            f\"_{cfg.cell_type.lower()}_do{int(cfg.dropout_p*100)}\"\n            f\"_beam{cfg.beam_width}_tf{int(cfg.teacher_forcing_ratio*100)}\"\n        )\n        wandb.run.name = run_name  # Update name after init\n\n        # Load data + build vocab\n        train_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n            BASE_DIR, batch_size=64, build_vocab=True\n        )\n        IDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}\n\n        # Initialize model\n        model = Seq2Seq(\n            input_size=src_size,\n            output_size=tgt_size,\n            embedding_dim=cfg.embedding_dim,\n            hidden_size=cfg.hidden_size,\n            encoder_layers=cfg.encoder_layers,\n            decoder_layers=cfg.decoder_layers,\n            cell_type=cfg.cell_type,\n            dropout_p=cfg.dropout_p,\n            bidirectional_encoder=False\n        ).to(DEVICE)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        criterion = nn.NLLLoss(ignore_index=pad_idx)\n        stopper = EarlyStopper(patience=5)\n        best_val_acc = 0.0\n\n        for epoch in range(1, 11):\n            model.train()\n            total_loss = 0.0\n            for src, tgt in tqdm(train_loader, desc=f\"[Epoch {epoch}] Training\", leave=False):\n                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                optimizer.zero_grad()\n                out = model(src, tgt, teacher_forcing_ratio=cfg.teacher_forcing_ratio)\n                loss = criterion(out.view(-1, tgt_size), tgt.view(-1))\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            # Validation\n            model.eval()\n            correct_seqs, total_seqs = 0, 0\n            with torch.no_grad():\n                for src, tgt in tqdm(val_loader, desc=f\"[Epoch {epoch}] Validation\", leave=False):\n                    src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                    out = model(src, tgt, teacher_forcing_ratio=0.0)\n                    preds = out.argmax(dim=2)\n                    for pred_seq, true_seq in zip(preds, tgt):\n                        pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                        true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                        if torch.equal(pred_tokens, true_tokens):\n                            correct_seqs += 1\n                        total_seqs += 1\n\n            val_acc = correct_seqs / total_seqs\n            wandb.log({'epoch': epoch, 'val_accuracy': val_acc, 'train_loss': total_loss})\n\n            print(f\"[Epoch {epoch}] Train Loss: {total_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n            elif stopper.should_stop(val_acc):\n                print(\"Early stopping triggered.\")\n                break\n\n        # Final test evaluation\n        model.eval()\n        correct_seqs, total_seqs = 0, 0\n        with torch.no_grad():\n            for src, tgt in tqdm(test_loader, desc=\"Final Test Eval\", leave=False):\n                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                out = model(src, tgt, teacher_forcing_ratio=0.0)\n                preds = out.argmax(dim=2)\n                for pred_seq, true_seq in zip(preds, tgt):\n                    pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                    true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                    if torch.equal(pred_tokens, true_tokens):\n                        correct_seqs += 1\n                    total_seqs += 1\n\n        test_acc = correct_seqs / total_seqs\n        print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n        wandb.log({'final_test_accuracy': test_acc})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:20:26.664504Z","iopub.execute_input":"2025-05-20T19:20:26.665158Z","iopub.status.idle":"2025-05-20T19:20:26.677378Z","shell.execute_reply.started":"2025-05-20T19:20:26.665135Z","shell.execute_reply":"2025-05-20T19:20:26.676553Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project='da6401_assignment3')\nwandb.agent(sweep_id, function=train, count=100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CHAR2IDX_SRC = {\n    \"<pad>\": 0,\n    \"<sos>\": 1,\n    \"<eos>\": 2,\n    \"<unk>\": 3,\n    **{c: i + 4 for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n}\nIDX2CHAR_SRC = {i: c for c, i in CHAR2IDX_SRC.items()}\n\n# Load data, build vocabs, and create reverse target-char map\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\n\nIDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}  # Map decoder indices back to Telugu chars\n\n# Model, optimizer, loss, early stopping\n\n# parameters of best model\nbest_model = Seq2Seq(\n    input_size=src_size,\n    output_size=tgt_size,\n    embedding_dim=64,\n    hidden_size=256,\n    encoder_layers=3,\n    decoder_layers=1,\n    cell_type='LSTM',  # or 'GRU' or 'RNN'\n    dropout_p=0.4,\n    bidirectional_encoder=False\n).to(DEVICE)\n\noptimizer = torch.optim.Adam(best_model.parameters(), lr=1e-3)\ncriterion = nn.NLLLoss(ignore_index=pad_idx)\nstopper = EarlyStopper(patience=5)\nbest_val_acc = 0.0\n\n# Training Loop\n\nfor epoch in range(1, 11):\n    best_model.train()\n    total_loss = 0.0\n    for src, tgt in tqdm(train_loader, desc=f\"[Epoch {epoch}] Training\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        optimizer.zero_grad()\n        out = best_model(src, tgt, teacher_forcing_ratio=1.0)\n        loss = criterion(out.view(-1, tgt_size), tgt.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    # Validation: sequence-level accuracy\n    best_model.eval()\n    correct_seqs, total_seqs = 0, 0\n    with torch.no_grad():\n        for src, tgt in tqdm(val_loader, desc=f\"[Epoch {epoch}] Validation\", leave=False):\n            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n            out = best_model(src, tgt, teacher_forcing_ratio=0.0)\n            preds = out.argmax(dim=2)\n            for pred_seq, true_seq in zip(preds, tgt):\n                # Remove <sos> and padding tokens for comparison\n                pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                if torch.equal(pred_tokens, true_tokens):\n                    correct_seqs += 1\n                total_seqs += 1\n\n    val_acc = correct_seqs / total_seqs\n    print(f\"[Epoch {epoch}] Loss: {total_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # Optionally save model checkpoint here\n    elif stopper.should_stop(val_acc):\n        print(\"Early stopping triggered.\")\n        break\n\n# Final Test Evaluation\nbest_model.eval()\ncorrect_seqs, total_seqs = 0, 0\nall_preds, all_trues = [], []\n\nwith torch.no_grad():\n    for src, tgt in tqdm(test_loader, desc=\"Final Test Eval\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out = best_model(src, tgt, teacher_forcing_ratio=0.0)\n        preds = out.argmax(dim=2)\n        for pred_seq, true_seq in zip(preds, tgt):\n            pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n            true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n            if torch.equal(pred_tokens, true_tokens):\n                correct_seqs += 1\n            total_seqs += 1\n\n            all_preds.append(pred_tokens)\n            all_trues.append(true_tokens)\n\ntest_acc = correct_seqs / total_seqs\nprint(f\"\\n Final Test Accuracy (Exact Word match): {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"romanized_test_words = []\nwith open(os.path.join(BASE_DIR, 'te.translit.sampled.test.tsv'), \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        telugu, roman, _ = line.strip().split()\n        romanized_test_words.append(roman)\n\nbest_model.eval()\nsamples = []\n\nwith torch.no_grad():\n    for i, (src, tgt) in enumerate(test_loader):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out = best_model(src, tgt, teacher_forcing_ratio=0.0)\n        preds = out.argmax(dim=2)\n\n        for b in range(src.shape[0]):\n            pred = ''.join(\n                IDX2CHAR_TGT[idx.item()] \n                for idx in preds[b][1:] \n                if idx.item() not in (pad_idx,)\n            )\n            true = ''.join(\n                IDX2CHAR_TGT[idx.item()] \n                for idx in tgt[b][1:] \n                if idx.item() not in (pad_idx,)\n            )\n\n            romanized = romanized_test_words[i * src.shape[0] + b]\n\n            samples.append({\n                'Input': romanized,\n                'True Telugu'             : true,\n                'Predicted Telugu'        : pred\n            })\n\n# Sample and show table\nsubset = random.sample(samples, min(10, len(samples)))\ndf = pd.DataFrame(subset)\nprint(df.to_markdown(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:28:32.996799Z","iopub.execute_input":"2025-05-20T19:28:32.997263Z","iopub.status.idle":"2025-05-20T19:29:14.648701Z","shell.execute_reply.started":"2025-05-20T19:28:32.997243Z","shell.execute_reply":"2025-05-20T19:29:14.647979Z"}},"outputs":[{"name":"stdout","text":"| Input            | True Telugu   | Predicted Telugu   |\n|:-----------------|:--------------|:-------------------|\n| gorinka          | గోరింక<eos>      | గొరింక్                |\n| palakadaaniki    | పలకడానికి<eos>   | పలకడానికి<eos>        |\n| lagnastha        | లగ్నస్థ<eos>    | లగ్నస్థ<eos>         |\n| jaanapadha       | జానపద<eos>     | జానపద్               |\n| poenichchadu     | పోనిచ్చాడు<eos>    | పోనిచ్చాడు<eos>         |\n| uttharayanam     | ఉత్తరాయణము<eos>  | ఉత్తరాయణం<eos><eos>   |\n| kemerala         | కెమెరాల<eos>     | కేమలరా<eos><eos>     |\n| sambhaashanalalo | సంభాషణలలో<eos>   | సంభాషణలలో<eos>        |\n| caryalatoo       | చర్యలతో<eos>    | చర్యలతో<eos>         |\n| thaati           | తాటి<eos>       | తాతి<eos>            |\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def highlight_pred(row):\n    \"\"\"\n    Returns a list of CSS styles, one per column, \n    coloring the 'Predicted Telugu' cell green if correct else red.\n    \"\"\"\n    styles = [''] * len(row)\n    # Find the index of the Predicted column\n    pred_idx = list(row.index).index('Predicted Telugu')\n    if row['Predicted Telugu'] == row['True Telugu']:\n        styles[pred_idx] = 'background-color: #c8e6c9; font-weight: bold;'  # light green\n    else:\n        styles[pred_idx] = 'background-color: #f8d7da; font-weight: bold;'  # light red\n    return styles\n\n# Apply to a random sample of 10 rows\nsubset = df.sample(n=min(10, len(df))).reset_index(drop=True)\n\nstyled = (\n    subset.style\n          .apply(highlight_pred, axis=1)\n          .set_table_styles([\n              # Center all text\n              {'selector': 'td, th',\n               'props': [('text-align', 'center'), ('padding', '6px')]},\n              # Header style\n              {'selector': 'th',\n               'props': [('background-color', '#4F81BD'),\n                         ('color', 'white'),\n                         ('font-weight', 'bold'),\n                         ('padding', '8px')]}\n          ])\n          .set_caption(\" Sample Transliteration Predictions (Green = Correct, Red = Wrong) \")\n)\n\n# Display in a Jupyter/HTML context\ndisplay(styled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:29:14.649365Z","iopub.execute_input":"2025-05-20T19:29:14.649543Z","iopub.status.idle":"2025-05-20T19:29:14.746247Z","shell.execute_reply.started":"2025-05-20T19:29:14.649529Z","shell.execute_reply":"2025-05-20T19:29:14.745535Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7af22b0902d0>","text/html":"<style type=\"text/css\">\n#T_b69f7 td {\n  text-align: center;\n  padding: 6px;\n}\n#T_b69f7  th {\n  text-align: center;\n  padding: 6px;\n}\n#T_b69f7 th {\n  background-color: #4F81BD;\n  color: white;\n  font-weight: bold;\n  padding: 8px;\n}\n#T_b69f7_row0_col2, #T_b69f7_row1_col2, #T_b69f7_row7_col2, #T_b69f7_row8_col2, #T_b69f7_row9_col2 {\n  background-color: #c8e6c9;\n  font-weight: bold;\n}\n#T_b69f7_row2_col2, #T_b69f7_row3_col2, #T_b69f7_row4_col2, #T_b69f7_row5_col2, #T_b69f7_row6_col2 {\n  background-color: #f8d7da;\n  font-weight: bold;\n}\n</style>\n<table id=\"T_b69f7\">\n  <caption>✨ Sample Transliteration Predictions (Green = Correct, Red = Wrong) ✨</caption>\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_b69f7_level0_col0\" class=\"col_heading level0 col0\" >Input</th>\n      <th id=\"T_b69f7_level0_col1\" class=\"col_heading level0 col1\" >True Telugu</th>\n      <th id=\"T_b69f7_level0_col2\" class=\"col_heading level0 col2\" >Predicted Telugu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_b69f7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_b69f7_row0_col0\" class=\"data row0 col0\" >sambhaashanalalo</td>\n      <td id=\"T_b69f7_row0_col1\" class=\"data row0 col1\" >సంభాషణలలో<eos></td>\n      <td id=\"T_b69f7_row0_col2\" class=\"data row0 col2\" >సంభాషణలలో<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_b69f7_row1_col0\" class=\"data row1 col0\" >poenichchadu</td>\n      <td id=\"T_b69f7_row1_col1\" class=\"data row1 col1\" >పోనిచ్చాడు<eos></td>\n      <td id=\"T_b69f7_row1_col2\" class=\"data row1 col2\" >పోనిచ్చాడు<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_b69f7_row2_col0\" class=\"data row2 col0\" >jaanapadha</td>\n      <td id=\"T_b69f7_row2_col1\" class=\"data row2 col1\" >జానపద<eos></td>\n      <td id=\"T_b69f7_row2_col2\" class=\"data row2 col2\" >జానపద్</td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_b69f7_row3_col0\" class=\"data row3 col0\" >uttharayanam</td>\n      <td id=\"T_b69f7_row3_col1\" class=\"data row3 col1\" >ఉత్తరాయణము<eos></td>\n      <td id=\"T_b69f7_row3_col2\" class=\"data row3 col2\" >ఉత్తరాయణం<eos><eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_b69f7_row4_col0\" class=\"data row4 col0\" >gorinka</td>\n      <td id=\"T_b69f7_row4_col1\" class=\"data row4 col1\" >గోరింక<eos></td>\n      <td id=\"T_b69f7_row4_col2\" class=\"data row4 col2\" >గొరింక్</td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_b69f7_row5_col0\" class=\"data row5 col0\" >thaati</td>\n      <td id=\"T_b69f7_row5_col1\" class=\"data row5 col1\" >తాటి<eos></td>\n      <td id=\"T_b69f7_row5_col2\" class=\"data row5 col2\" >తాతి<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_b69f7_row6_col0\" class=\"data row6 col0\" >kemerala</td>\n      <td id=\"T_b69f7_row6_col1\" class=\"data row6 col1\" >కెమెరాల<eos></td>\n      <td id=\"T_b69f7_row6_col2\" class=\"data row6 col2\" >కేమలరా<eos><eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_b69f7_row7_col0\" class=\"data row7 col0\" >lagnastha</td>\n      <td id=\"T_b69f7_row7_col1\" class=\"data row7 col1\" >లగ్నస్థ<eos></td>\n      <td id=\"T_b69f7_row7_col2\" class=\"data row7 col2\" >లగ్నస్థ<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_b69f7_row8_col0\" class=\"data row8 col0\" >palakadaaniki</td>\n      <td id=\"T_b69f7_row8_col1\" class=\"data row8 col1\" >పలకడానికి<eos></td>\n      <td id=\"T_b69f7_row8_col2\" class=\"data row8 col2\" >పలకడానికి<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_b69f7_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_b69f7_row9_col0\" class=\"data row9 col0\" >caryalatoo</td>\n      <td id=\"T_b69f7_row9_col1\" class=\"data row9 col1\" >చర్యలతో<eos></td>\n      <td id=\"T_b69f7_row9_col2\" class=\"data row9 col2\" >చర్యలతో<eos></td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Ensure the output folder exists\nos.makedirs(\"predictions_attention\", exist_ok=True)\n\ndf1 = pd.DataFrame(samples)\ndf1.to_csv(\"predictions_attention/predictions_attention.csv\", index=False, encoding=\"utf-8-sig\")\n\nprint(\" Saved all predictions to predictions_attention/predictions_attention.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:30:08.288924Z","iopub.execute_input":"2025-05-20T19:30:08.289268Z","iopub.status.idle":"2025-05-20T19:30:08.325249Z","shell.execute_reply.started":"2025-05-20T19:30:08.289245Z","shell.execute_reply":"2025-05-20T19:30:08.324478Z"}},"outputs":[{"name":"stdout","text":" Saved all predictions to predictions_attention/predictions_attention.csv\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"wandb.init(project=\"da6401_assignment3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:15:26.133573Z","iopub.execute_input":"2025-05-20T15:15:26.134124Z","iopub.status.idle":"2025-05-20T15:15:32.753117Z","shell.execute_reply.started":"2025-05-20T15:15:26.134097Z","shell.execute_reply":"2025-05-20T15:15:32.752355Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_151526-po6tihd6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/po6tihd6' target=\"_blank\">fiery-bee-237</a></strong> to <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3' target=\"_blank\">https://wandb.ai/cs24m018-iitmaana/da6401_assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/po6tihd6' target=\"_blank\">https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/po6tihd6</a>"},"metadata":{}},{"execution_count":46,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/po6tihd6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7d4900964250>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"from matplotlib import rcParams\n\n# Set the font family to Noto Sans Telugu\nrcParams['font.family'] = ['Noto Sans', 'Noto Sans Telugu', 'sans-serif']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:26:09.043657Z","iopub.execute_input":"2025-05-20T15:26:09.044005Z","iopub.status.idle":"2025-05-20T15:26:09.048998Z","shell.execute_reply.started":"2025-05-20T15:26:09.043980Z","shell.execute_reply":"2025-05-20T15:26:09.048315Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Ensure output directory exists\nos.makedirs(\"predictions_vanilla/attention_maps\", exist_ok=True)\n\n# Put model in eval mode\nbest_model.eval()\n\nwandb_images = []\nnum_samples = 12\nsamples_collected = 0\n\nwith torch.no_grad():\n    for src, tgt in test_loader:\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out, attn_weights = best_model(src, tgt, return_attention=True)\n\n        for b in range(src.size(0)):\n            # Decode source and target tokens\n            src_tokens = [IDX2CHAR_SRC[i.item()] for i in src[b] if i.item() != pad_idx]\n            tgt_tokens = [IDX2CHAR_TGT[i.item()] for i in tgt[b][1:] if i.item() != pad_idx]\n\n            # Get predicted tokens\n            pred_tokens = out.argmax(dim=2)[b][1:len(tgt_tokens)+1]\n            pred_chars = [IDX2CHAR_TGT[i.item()] for i in pred_tokens]\n\n            # Attention weights: [pred_len, src_len]\n            attn = attn_weights[b][:len(pred_chars), :len(src_tokens)]\n\n            # Plot heatmap\n            fig, ax = plt.subplots(figsize=(6, 4))\n            sns.heatmap(attn.cpu().numpy(), \n                        xticklabels=src_tokens, \n                        yticklabels=pred_chars, \n                        cmap='viridis', \n                        cbar=False, \n                        annot=False, \n                        linewidths=0.5, \n                        ax=ax)\n            ax.set_xlabel(\"Input\")\n            ax.set_ylabel(\"Predicted Output (Telugu)\")\n            ax.set_title(f\"Attention Heatmap {samples_collected + 1}\")\n            plt.tight_layout()\n\n            # Save figure\n            path = f\"predictions_vanilla/attention_maps/sample_{samples_collected + 1}.png\"\n            fig.savefig(path)\n            plt.close(fig)\n\n            # Log to wandb\n            wandb_images.append(wandb.Image(path, caption=f\"Sample {samples_collected + 1}\"))\n\n            samples_collected += 1\n            if samples_collected >= num_samples:\n                break\n        if samples_collected >= num_samples:\n            break\n\n# Optionally log all images to wandb\nwandb.log({\"attention_maps\": wandb_images})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:26:09.428287Z","iopub.execute_input":"2025-05-20T15:26:09.428543Z","iopub.status.idle":"2025-05-20T15:26:11.368383Z","shell.execute_reply.started":"2025-05-20T15:26:09.428525Z","shell.execute_reply":"2025-05-20T15:26:11.367606Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:26:24.449003Z","iopub.execute_input":"2025-05-20T15:26:24.449507Z","iopub.status.idle":"2025-05-20T15:26:25.059628Z","shell.execute_reply.started":"2025-05-20T15:26:24.449480Z","shell.execute_reply":"2025-05-20T15:26:25.059102Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fiery-bee-237</strong> at: <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/po6tihd6' target=\"_blank\">https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/po6tihd6</a><br> View project at: <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3' target=\"_blank\">https://wandb.ai/cs24m018-iitmaana/da6401_assignment3</a><br>Synced 5 W&B file(s), 36 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_151526-po6tihd6/logs</code>"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"wandb.init(project=\"da6401_assignment3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:30:58.830391Z","iopub.execute_input":"2025-05-20T16:30:58.831075Z","iopub.status.idle":"2025-05-20T16:31:05.480621Z","shell.execute_reply.started":"2025-05-20T16:30:58.831050Z","shell.execute_reply":"2025-05-20T16:31:05.480048Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_163058-oj48ik8s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/oj48ik8s' target=\"_blank\">curious-pyramid-244</a></strong> to <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3' target=\"_blank\">https://wandb.ai/cs24m018-iitmaana/da6401_assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/oj48ik8s' target=\"_blank\">https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/oj48ik8s</a>"},"metadata":{}},{"execution_count":75,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/oj48ik8s?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7d48ce60f090>"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"best_model.eval()\nall_samples = []\n\n# First, collect all model outputs\nwith torch.no_grad():\n    for i, (src, tgt) in enumerate(test_loader):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out, attn_weights = best_model(src, tgt, return_attention=True)\n        batch_size = src.size(0)\n\n        for b in range(batch_size):\n            romanized = romanized_test_words[i * batch_size + b]\n            src_tokens = list(romanized)\n            tgt_tokens = [IDX2CHAR_TGT[idx.item()] for idx in tgt[b][1:] if idx.item() != pad_idx]\n            pred_tokens = out.argmax(dim=2)[b][1:len(tgt_tokens)+1]\n            pred_chars = [IDX2CHAR_TGT[idx.item()] for idx in pred_tokens]\n\n            attn = attn_weights[b][:len(pred_chars), :len(src_tokens)].cpu().numpy().tolist()\n\n            all_samples.append((src_tokens, pred_chars, attn))\n\n# Select 10 random samples\nrandom_samples = random.sample(all_samples, 10)\n\n# Build HTML blocks\nhtml_blocks = []\nfor sample_count, (src_tokens, pred_chars, attn) in enumerate(random_samples):\n    input_tokens_js = json.dumps(src_tokens, ensure_ascii=False)\n    output_tokens_js = json.dumps(pred_chars, ensure_ascii=False)\n    attention_js = json.dumps(attn)\n\n    html_block = f\"\"\"\n    <div style=\"margin-bottom: 50px;\">\n      <h2>Sample {sample_count + 1}</h2>\n      <div><strong>Input (English):</strong></div>\n      <div id=\"input-tokens-{sample_count}\"></div>\n      <div><strong>Predicted Output (Telugu):</strong></div>\n      <div id=\"output-tokens-{sample_count}\"></div>\n      <script>\n        const inputTokens_{sample_count} = {input_tokens_js};\n        const outputTokens_{sample_count} = {output_tokens_js};\n        const attention_{sample_count} = {attention_js};\n\n        const inputDiv_{sample_count} = d3.select(\"#input-tokens-{sample_count}\");\n        const outputDiv_{sample_count} = d3.select(\"#output-tokens-{sample_count}\");\n\n        inputTokens_{sample_count}.forEach((token, i) => {{\n          inputDiv_{sample_count}.append(\"span\")\n            .attr(\"class\", \"token input\")\n            .attr(\"id\", \"input-{sample_count}-\" + i)\n            .text(token);\n        }});\n\n        outputTokens_{sample_count}.forEach((token, i) => {{\n          outputDiv_{sample_count}.append(\"span\")\n            .attr(\"class\", \"token output\")\n            .text(token)\n            .on(\"mouseover\", () => {{\n              d3.selectAll(\".token.input\").style(\"background-color\", \"#fff\");\n              attention_{sample_count}[i].forEach((score, j) => {{\n                const color = d3.interpolateOranges(score);\n                d3.select(\"#input-{sample_count}-\" + j).style(\"background-color\", color);\n              }});\n            }})\n            .on(\"mouseout\", () => {{\n              d3.selectAll(\".token.input\").style(\"background-color\", \"#fff\");\n            }});\n        }});\n      </script>\n    </div>\n    \"\"\"\n    html_blocks.append(html_block)\n\n# Full HTML document\nfull_html = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <title>Attention Visualizations</title>\n  <script src=\"https://d3js.org/d3.v7.min.js\"></script>\n  <style>\n    body {{ font-family: Arial, sans-serif; margin: 30px; }}\n    .token {{\n      display: inline-block;\n      padding: 8px 12px;\n      margin: 3px;\n      border-radius: 5px;\n      border: 1px solid #ccc;\n      font-size: 20px;\n      cursor: pointer;\n      user-select: none;\n      transition: background-color 0.3s;\n    }}\n  </style>\n</head>\n<body>\n  <h1>Random Attention Visualizations (10 Samples)</h1>\n  {''.join(html_blocks)}\n</body>\n</html>\n\"\"\"\n\n# Log to WandB\nwandb.log({\"attention_visualizations_random_10\": wandb.Html(full_html)})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:31:05.481867Z","iopub.execute_input":"2025-05-20T16:31:05.482098Z","iopub.status.idle":"2025-05-20T16:31:49.221276Z","shell.execute_reply.started":"2025-05-20T16:31:05.482075Z","shell.execute_reply":"2025-05-20T16:31:49.220750Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:31:49.221973Z","iopub.execute_input":"2025-05-20T16:31:49.222177Z","iopub.status.idle":"2025-05-20T16:31:49.850825Z","shell.execute_reply.started":"2025-05-20T16:31:49.222161Z","shell.execute_reply":"2025-05-20T16:31:49.850270Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">curious-pyramid-244</strong> at: <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/oj48ik8s' target=\"_blank\">https://wandb.ai/cs24m018-iitmaana/da6401_assignment3/runs/oj48ik8s</a><br> View project at: <a href='https://wandb.ai/cs24m018-iitmaana/da6401_assignment3' target=\"_blank\">https://wandb.ai/cs24m018-iitmaana/da6401_assignment3</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_163058-oj48ik8s/logs</code>"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}