{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11850506,"sourceType":"datasetVersion","datasetId":7446208}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport os\nimport wandb\nfrom tqdm import tqdm\nimport re\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:26.503135Z","iopub.execute_input":"2025-05-20T19:58:26.503349Z","iopub.status.idle":"2025-05-20T19:58:33.900505Z","shell.execute_reply.started":"2025-05-20T19:58:26.503334Z","shell.execute_reply":"2025-05-20T19:58:33.900008Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_key\")\n\nos.environ['WANDB_API_KEY'] = secret_value_0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:33.901160Z","iopub.execute_input":"2025-05-20T19:58:33.901457Z","iopub.status.idle":"2025-05-20T19:58:34.155945Z","shell.execute_reply.started":"2025-05-20T19:58:33.901440Z","shell.execute_reply":"2025-05-20T19:58:34.155237Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# DEVICE\nDEVICE   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBASE_DIR = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/te/lexicons' # Change 'te' → desired language","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.156654Z","iopub.execute_input":"2025-05-20T19:58:34.156913Z","iopub.status.idle":"2025-05-20T19:58:34.213518Z","shell.execute_reply.started":"2025-05-20T19:58:34.156889Z","shell.execute_reply":"2025-05-20T19:58:34.212765Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CharacterEmbedding(nn.Module):\n    # Creating an embedding layer that maps input character indices to embedding vectors.\n    # input_size: number of unique characters (vocabulary size)\n    # embedding_dim: size of each embedding vector\n    def __init__(self, input_size, embedding_dim):\n        super(CharacterEmbedding, self).__init__()\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n\n    # Returns corresponding embedding vectors of shape (batch_size, seq_length, embedding_dim)\n    def forward(self, input_seq):\n        # input_seq: a tensor of character indices, typically of shape (batch_size, seq_length)\n        return self.embedding(input_seq)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.215581Z","iopub.execute_input":"2025-05-20T19:58:34.215798Z","iopub.status.idle":"2025-05-20T19:58:34.230495Z","shell.execute_reply.started":"2025-05-20T19:58:34.215781Z","shell.execute_reply":"2025-05-20T19:58:34.229781Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# EncoderRNN transforms sequences of token IDs into contextual hidden states\n# Supports GRU, LSTM, or vanilla RNN cells\n# input_size: number of unique tokens\n# hidden_size: size of the RNN hidden state\n# embedding_dim: size of token embedding vectors\n# num_layers: number of stacked recurrent layers\n# cell_type: 'GRU', 'LSTM', or 'RNN'\n# dropout_p: dropout probability between RNN layers (only if num_layers > 1)\n# bidirectional: whether to run the RNN in both forward and backward directions\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, embedding_dim, num_layers=1,cell_type='GRU', dropout_p=0.1, bidirectional=False):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional\n        self.directions = 2 if bidirectional else 1\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        \n        # Dropout before the RNN (applied to embeddings)\n        self.dropout = nn.Dropout(dropout_p)\n        dropout_p = dropout_p if num_layers > 1 else 0\n        \n        # RNN layer\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, batch_first=True)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, batch_first=True)\n        else:  # Default to RNN\n            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers,dropout=dropout_p,bidirectional=bidirectional, nonlinearity='tanh', batch_first=True)\n\n    # Forward pass through the encoder\n    def forward(self, input_seq):\n        # Input shape: [batch_size, seq_len]\n        batch_size = input_seq.size(0)\n        \n        # Convert indices to embeddings and apply dropout to embeddings\n        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embedding_dim]\n        embedded = self.dropout(embedded)\n        \n        # Pass through RNN\n        outputs, hidden = self.rnn(embedded)\n        \n        return outputs, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.231234Z","iopub.execute_input":"2025-05-20T19:58:34.231574Z","iopub.status.idle":"2025-05-20T19:58:34.246352Z","shell.execute_reply.started":"2025-05-20T19:58:34.231551Z","shell.execute_reply":"2025-05-20T19:58:34.245643Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# DecoderRNN generates target sequences one token at a time\nclass DecoderRNN(nn.Module):\n    def __init__(self, output_size, hidden_size, embedding_dim, num_layers=1, \n                 cell_type='GRU', dropout_p=0.1):\n        \n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        \n        # Embedding layer for target characters\n        self.embedding = nn.Embedding(output_size, embedding_dim)\n        \n        # Dropout applied before RNN\n        self.dropout = nn.Dropout(dropout_p)\n\n        dropout_p = dropout_p if num_layers > 1 else 0\n        \n        # RNN layer\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers,dropout=dropout_p,batch_first=True)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers,dropout=dropout_p,batch_first=True)\n        else:  # Default to RNN\n            self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers,dropout=dropout_p,nonlinearity='tanh', batch_first=True)\n            \n        # Output projection layer with dropout\n        self.out_dropout = nn.Dropout(dropout_p)\n        self.out = nn.Linear(hidden_size, output_size)\n\n    # Forward pass for a single decoding step\n    def forward(self, input_char, hidden):\n        # Convert input to embeddings and apply dropout\n        embedded = self.embedding(input_char)  # [batch_size, 1, embedding_dim]\n        embedded = self.dropout(embedded)\n        \n        # Pass through RNN\n        output, hidden = self.rnn(embedded, hidden)\n        \n        # Apply dropout before prediction layer\n        output = self.out_dropout(output)\n        output = self.out(output[:, 0, :])\n        \n        return F.log_softmax(output, dim=1), hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.247097Z","iopub.execute_input":"2025-05-20T19:58:34.247383Z","iopub.status.idle":"2025-05-20T19:58:34.265554Z","shell.execute_reply.started":"2025-05-20T19:58:34.247362Z","shell.execute_reply":"2025-05-20T19:58:34.264867Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#  Perform beam search decoding with the trained seq2seq model.\ndef beam_search_decode(model, src, sos_idx, eos_idx, max_len=30, beam_width=3, device='gpu'):\n    model.eval()\n    with torch.no_grad():\n        # Encode input\n        encoder_outputs, encoder_hidden = model.encoder(src)\n\n        # Prepare initial decoder hidden state\n        if model.bidirectional:\n            if model.cell_type == 'LSTM':\n                h_n, c_n = encoder_hidden\n                h_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size).to(device)\n                c_dec = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size).to(device)\n                for layer in range(model.encoder.num_layers):\n                    h_combined = torch.cat((h_n[2*layer], h_n[2*layer+1]), dim=1)\n                    c_combined = torch.cat((c_n[2*layer], c_n[2*layer+1]), dim=1)\n                    h_dec[layer] = model.hidden_transform(h_combined)\n                    c_dec[layer] = model.hidden_transform(c_combined)\n                decoder_hidden = (h_dec, c_dec)\n            else:\n                decoder_hidden = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size).to(device)\n                for layer in range(model.encoder.num_layers):\n                    h_combined = torch.cat((encoder_hidden[2*layer], encoder_hidden[2*layer+1]), dim=1)\n                    decoder_hidden[layer] = model.hidden_transform(h_combined)\n        else:\n            decoder_hidden = encoder_hidden\n\n        # Beam search initialization\n        beams = [([sos_idx], 0.0, decoder_hidden)]  # (sequence, cumulative log-prob, hidden)\n        completed = []\n\n        for _ in range(max_len):\n            new_beams = []\n            for seq, score, hidden in beams:\n                if seq[-1] == eos_idx:\n                    completed.append((seq, score))\n                    continue\n                input_char = torch.tensor([[seq[-1]]], device=device)\n                output, hidden_new = model.decoder(input_char, hidden)\n                log_probs = output.squeeze(0)  # [output_size]\n                topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n                for k in range(beam_width):\n                    next_seq = seq + [topk_indices[k].item()]\n                    next_score = score + topk_log_probs[k].item()\n                    new_beams.append((next_seq, next_score, hidden_new))\n            # Keep top beam_width beams\n            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n            if not beams:\n                break\n\n        # Add any remaining beams\n        completed += [(seq, score) for seq, score, _ in beams if seq[-1] == eos_idx]\n        # If none ended with <eos>, just take the best\n        if not completed:\n            completed = beams\n\n        # Sort by score\n        completed = sorted(completed, key=lambda x: x[1], reverse=True)\n        return completed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.266326Z","iopub.execute_input":"2025-05-20T19:58:34.266525Z","iopub.status.idle":"2025-05-20T19:58:34.289026Z","shell.execute_reply.started":"2025-05-20T19:58:34.266486Z","shell.execute_reply":"2025-05-20T19:58:34.288345Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Seq2Seq implements an Encoder and Decoder for end-to-end sequence-to-sequence modeling\n# input_size: size of source vocabulary\n# output_size: size of target vocabulary\n# embedding_dim: dimension of embeddings in both encoder and decoder\n# hidden_size: size of hidden states in encoder and decoder (must match for vanilla seq2seq)\n# encoder_layers / decoder_layers: number of stacked RNN layers\n# cell_type: 'GRU', 'LSTM', or 'RNN'\n# dropout_p: dropout probability for embeddings and RNN layers\n# bidirectional_encoder: if True, runs encoder bidirectionally and transforms hidden state\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, input_size, output_size, embedding_dim=256, hidden_size=256,\n                 encoder_layers=1, decoder_layers=1, cell_type='GRU', dropout_p=0.2,\n                 bidirectional_encoder=False):\n        super(Seq2Seq, self).__init__()\n        \n        # Create encoder RNN\n        self.encoder = EncoderRNN(input_size, hidden_size, embedding_dim,\n                                  num_layers=encoder_layers, cell_type=cell_type,\n                                  dropout_p=dropout_p, bidirectional=bidirectional_encoder)\n        \n        self.bidirectional = bidirectional_encoder\n        directions = 2 if bidirectional_encoder else 1\n        \n        # If bidirectional encoder, need a linear layer to transform hidden state\n        if bidirectional_encoder:\n            self.hidden_transform = nn.Linear(hidden_size * directions, hidden_size)\n        \n        # Create decoder RNN\n        self.decoder = DecoderRNN(output_size, hidden_size, embedding_dim,\n                                 num_layers=decoder_layers, cell_type=cell_type,\n                                 dropout_p=dropout_p)\n        \n        self.cell_type = cell_type\n\n    def _match_decoder_layers(self, hidden, batch_size):\n        \"\"\"Ensures hidden state matches decoder layers by trimming or padding.\"\"\"\n        if hidden.size(0) > self.decoder.num_layers:\n            return hidden[:self.decoder.num_layers]\n        elif hidden.size(0) < self.decoder.num_layers:\n            pad = torch.zeros(self.decoder.num_layers - hidden.size(0),\n                              batch_size, self.decoder.hidden_size,\n                              device=hidden.device)\n            return torch.cat([hidden, pad], dim=0)\n        else:\n            return hidden\n        \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        output_size = self.decoder.output_size\n        \n        # Tensor to store decoder outputs (logits or log-probs)\n        outputs = torch.zeros(batch_size, trg_len, output_size).to(src.device)\n        \n        # Encode the source sequence\n        encoder_outputs, encoder_hidden = self.encoder(src)\n        decoder_hidden = None \n        \n        # Prepare initial hidden state for decoder\n        if self.bidirectional:\n            # Bidirectional encoder returns hidden states with doubled layers * 2 directions\n            \n            if self.cell_type == 'LSTM':\n                # For LSTM, hidden state is a tuple (h_n, c_n)\n                h_n, c_n = encoder_hidden\n                \n                # Initialize decoder hidden states\n                h_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(src.device)\n                c_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(src.device)\n                \n                # For each decoder layer, combine corresponding forward and backward encoder layers\n                for layer in range(self.decoder.num_layers):\n                    # Clamp to max encoder layers index to avoid index errors if decoder has more layers\n                    enc_layer = min(layer, self.encoder.num_layers - 1)\n                    \n                    # Concatenate forward and backward hidden states from encoder for this layer\n                    h_combined = torch.cat((h_n[2 * enc_layer], h_n[2 * enc_layer + 1]), dim=1)\n                    c_combined = torch.cat((c_n[2 * enc_layer], c_n[2 * enc_layer + 1]), dim=1)\n                    \n                    # Transform concatenated states to decoder hidden size\n                    h_dec[layer] = self.hidden_transform(h_combined)\n                    c_dec[layer] = self.hidden_transform(c_combined)\n                \n                decoder_hidden = (h_dec, c_dec)\n            \n            else:\n                # For GRU or vanilla RNN (hidden state is a single tensor)\n                h_n = encoder_hidden\n                h_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(src.device)\n                \n                for layer in range(self.decoder.num_layers):\n                    enc_layer = min(layer, self.encoder.num_layers - 1)\n                    h_combined = torch.cat((h_n[2 * enc_layer], h_n[2 * enc_layer + 1]), dim=1)\n                    h_dec[layer] = self.hidden_transform(h_combined)\n                \n                decoder_hidden = h_dec\n        \n        else:\n            if self.cell_type == \"LSTM\":\n                h, c = encoder_hidden\n                decoder_hidden = (\n                    self._match_decoder_layers(h, batch_size),\n                    self._match_decoder_layers(c, batch_size)\n                )\n            else:\n                decoder_hidden = self._match_decoder_layers(encoder_hidden, batch_size)\n        \n        # First input to decoder is <sos> token from target\n        input_char = trg[:, 0].unsqueeze(1)  # shape: (batch_size, 1)\n        \n        # Decode one token at a time\n        for t in range(1, trg_len):\n            output, decoder_hidden = self.decoder(input_char, decoder_hidden)\n            outputs[:, t, :] = output\n            \n            # Decide if teacher forcing should be used\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            # Get highest probability token from output\n            top1 = output.argmax(1).unsqueeze(1)\n            \n            # Next input is either true target token or predicted token\n            input_char = trg[:, t].unsqueeze(1) if teacher_force else top1\n        \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.289825Z","iopub.execute_input":"2025-05-20T19:58:34.290097Z","iopub.status.idle":"2025-05-20T19:58:34.311538Z","shell.execute_reply.started":"2025-05-20T19:58:34.290073Z","shell.execute_reply":"2025-05-20T19:58:34.310921Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class LexiconDataset(Dataset):\n    def __init__(self, path, src_vocab=None, tgt_vocab=None, build_vocab=False):\n        self.pairs = []\n        with open(path, encoding='utf-8') as f:\n            for line in f:\n                cols = line.strip().split('\\t')\n                if len(cols) < 2:\n                    continue\n                tgt, src = cols[0], cols[1]  # Telugu is first column, romanized second\n                self.pairs.append((src, tgt)) # src = romanized, tgt = telugu\n\n        if build_vocab:\n            self.src_vocab = {'<pad>':0, '<sos>':1, '<eos>':2, '<unk>':3}\n            self.tgt_vocab = {'<pad>':0, '<sos>':1, '<eos>':2, '<unk>':3}\n            for rom, dev in self.pairs:\n                for c in rom:\n                    self.src_vocab.setdefault(c, len(self.src_vocab))\n                for c in dev:\n                    self.tgt_vocab.setdefault(c, len(self.tgt_vocab))\n        else:\n            assert src_vocab and tgt_vocab, \"Must provide vocabs if not building.\"\n            self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        rom, dev = self.pairs[idx]\n        src_idxs = [self.src_vocab.get(c, self.src_vocab['<unk>']) for c in rom]\n        tgt_idxs = [self.tgt_vocab['<sos>']] + [self.tgt_vocab.get(c, self.tgt_vocab['<unk>']) for c in dev] + [self.tgt_vocab['<eos>']]\n        return torch.tensor(src_idxs, dtype=torch.long), torch.tensor(tgt_idxs, dtype=torch.long)\n\ndef collate_fn(batch):\n    \"\"\"\n    Pads all src/tgt sequences in the batch to the max length.\n    Returns:\n      padded_src: (batch_size, max_src_len)\n      padded_tgt: (batch_size, max_tgt_len)\n    \"\"\"\n    srcs, tgts = zip(*batch)\n    max_src = max(len(s) for s in srcs)\n    max_tgt = max(len(t) for t in tgts)\n\n    padded_src = torch.full((len(batch), max_src), 0, dtype=torch.long)\n    padded_tgt = torch.full((len(batch), max_tgt), 0, dtype=torch.long)\n    for i, (s, t) in enumerate(zip(srcs, tgts)):\n        padded_src[i, :len(s)] = s\n        padded_tgt[i, :len(t)] = t\n\n    return padded_src, padded_tgt\n\ndef get_dataloaders(base_dir, batch_size, build_vocab=False):\n    \"\"\"\n    Returns:\n      train_loader, val_loader, test_loader,\n      src_vocab_size, tgt_vocab_size, pad_index, src_vocab, tgt_vocab\n    \"\"\"\n    train_p = os.path.join(base_dir, 'te.translit.sampled.train.tsv')\n    dev_p   = os.path.join(base_dir, 'te.translit.sampled.dev.tsv')\n    test_p  = os.path.join(base_dir, 'te.translit.sampled.test.tsv')\n\n    train_ds = LexiconDataset(train_p, build_vocab=build_vocab)\n    src_vocab, tgt_vocab = train_ds.src_vocab, train_ds.tgt_vocab\n    val_ds   = LexiconDataset(dev_p,  src_vocab, tgt_vocab)\n    test_ds  = LexiconDataset(test_p, src_vocab, tgt_vocab)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader  = DataLoader(test_ds,  batch_size=1,            shuffle=False, collate_fn=collate_fn)\n\n    return (train_loader, val_loader, test_loader,\n            len(src_vocab), len(tgt_vocab), src_vocab['<pad>'],\n            src_vocab, tgt_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.312339Z","iopub.execute_input":"2025-05-20T19:58:34.312704Z","iopub.status.idle":"2025-05-20T19:58:34.332823Z","shell.execute_reply.started":"2025-05-20T19:58:34.312682Z","shell.execute_reply":"2025-05-20T19:58:34.332148Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class EarlyStopper:\n    \"\"\"Stops a run if the monitored metric doesn’t improve for `patience` steps.\"\"\"\n    def __init__(self, patience=5, min_delta=1e-4):\n        self.patience, self.min_delta = patience, min_delta\n        self.counter, self.best = 0, None\n\n    def should_stop(self, current):\n        if self.best is None or current > self.best + self.min_delta:\n            self.best, self.counter = current, 0\n        else:\n            self.counter += 1\n        return self.counter >= self.patience","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.333479Z","iopub.execute_input":"2025-05-20T19:58:34.333700Z","iopub.status.idle":"2025-05-20T19:58:34.350878Z","shell.execute_reply.started":"2025-05-20T19:58:34.333680Z","shell.execute_reply":"2025-05-20T19:58:34.350213Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"CHAR2IDX_SRC = {\n    \"<pad>\": 0,\n    \"<sos>\": 1,\n    \"<eos>\": 2,\n    \"<unk>\": 3,\n    **{c: i + 4 for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n}\nIDX2CHAR_SRC = {i: c for c, i in CHAR2IDX_SRC.items()}\n\n# Load data, build vocabs, and create reverse target-char map\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\n\nIDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}  # Map decoder indices back to Telugu chars\n\n# Model, optimizer, loss, early stopping\n\nmodel = Seq2Seq(\n    input_size=src_size,\n    output_size=tgt_size,\n    embedding_dim=64,\n    hidden_size=128,\n    encoder_layers=1,\n    decoder_layers=3,\n    cell_type='GRU',  # or 'GRU' or 'RNN'\n    dropout_p=0.3,\n    bidirectional_encoder=False\n).to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.NLLLoss(ignore_index=pad_idx)\nstopper = EarlyStopper(patience=5)\nbest_val_acc = 0.0\n\n# Training Loop\n\nfor epoch in range(1, 11):\n    model.train()\n    total_loss = 0.0\n    for src, tgt in tqdm(train_loader, desc=f\"[Epoch {epoch}] Training\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        optimizer.zero_grad()\n        out = model(src, tgt, teacher_forcing_ratio=0.7)\n        loss = criterion(out.view(-1, tgt_size), tgt.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    # Validation: sequence-level accuracy\n    model.eval()\n    correct_seqs, total_seqs = 0, 0\n    with torch.no_grad():\n        for src, tgt in tqdm(val_loader, desc=f\"[Epoch {epoch}] Validation\", leave=False):\n            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n            out = model(src, tgt, teacher_forcing_ratio=0.0)\n            preds = out.argmax(dim=2)\n            for pred_seq, true_seq in zip(preds, tgt):\n                # Remove <sos> and padding tokens for comparison\n                pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                if torch.equal(pred_tokens, true_tokens):\n                    correct_seqs += 1\n                total_seqs += 1\n\n    val_acc = correct_seqs / total_seqs\n    print(f\"[Epoch {epoch}] Loss: {total_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # Optionally save model checkpoint here\n    elif stopper.should_stop(val_acc):\n        print(\"Early stopping triggered.\")\n        break\n\n# Final Test Evaluation\nmodel.eval()\ncorrect_seqs, total_seqs = 0, 0\nall_preds, all_trues = [], []\n\nwith torch.no_grad():\n    for src, tgt in tqdm(test_loader, desc=\"Final Test Eval\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out = model(src, tgt, teacher_forcing_ratio=0.0)\n        preds = out.argmax(dim=2)\n        for pred_seq, true_seq in zip(preds, tgt):\n            pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n            true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n            if torch.equal(pred_tokens, true_tokens):\n                correct_seqs += 1\n            total_seqs += 1\n\n            all_preds.append(pred_tokens)\n            all_trues.append(true_tokens)\n\ntest_acc = correct_seqs / total_seqs\nprint(f\"\\n Final Test Accuracy: {test_acc:.4f}\")\n\n\n# Sample Predictions in Telugu\nprint(\"\\n Sample Predictions:\")\nsample_indices = random.sample(range(len(all_preds)), min(10, len(all_preds)))\nfor idx in sample_indices:\n    pred_str = ''.join([IDX2CHAR_TGT[token.item()] for token in all_preds[idx]])\n    true_str = ''.join([IDX2CHAR_TGT[token.item()] for token in all_trues[idx]])\n    correctness = \"true\" if pred_str == true_str else \"false\"\n    print(f\"  True : {true_str}\\n  Pred : {pred_str}  {correctness}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T18:18:53.475670Z","iopub.execute_input":"2025-05-19T18:18:53.475950Z","iopub.status.idle":"2025-05-19T18:24:57.970400Z","shell.execute_reply.started":"2025-05-19T18:18:53.475929Z","shell.execute_reply":"2025-05-19T18:24:57.969532Z"}},"outputs":[{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Loss: 2245.6370 | Val Acc: 0.0002\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Loss: 1503.1435 | Val Acc: 0.0707\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Loss: 1035.8125 | Val Acc: 0.1863\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Loss: 817.9509 | Val Acc: 0.2708\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Loss: 702.1377 | Val Acc: 0.3113\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Loss: 626.4294 | Val Acc: 0.3532\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Loss: 577.1637 | Val Acc: 0.3774\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Loss: 536.8485 | Val Acc: 0.4065\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Loss: 505.8226 | Val Acc: 0.4186\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Loss: 481.2055 | Val Acc: 0.4209\n","output_type":"stream"},{"name":"stderr","text":"                                                                     ","output_type":"stream"},{"name":"stdout","text":"\n Final Test Accuracy: 0.3108\n\n Sample Predictions:\n  True : అవసరము<eos>\n  Pred : అవసరము<eos>  true\n\n  True : తెలంగాణకు<eos>\n  Pred : తెలంగానకు<eos>  false\n\n  True : పోల్<eos>\n  Pred : పోలు<eos>  false\n\n  True : బలహీనత<eos>\n  Pred : బలహినత<eos>  false\n\n  True : ఆస్ట్రేలియా<eos>\n  Pred : అస్టర్లియా<eos><eos>  false\n\n  True : ఓటర్ల<eos>\n  Pred : ఓటర్ల<eos>  true\n\n  True : మీరూ<eos>\n  Pred : మీరుల  false\n\n  True : జాతీయోద్యమానికి<eos>\n  Pred : జతీయోయాయమిని<eos><eos><eos><eos>  false\n\n  True : కధలుగా<eos>\n  Pred : కథలుగా<eos>  false\n\n  True : చెందినదే<eos>\n  Pred : చెందినదే<eos>  true\n\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Define the sweep configuration\nsweep_config = {\n  'method': 'bayes',\n  'metric':     {'name': 'val_accuracy', 'goal': 'maximize'},\n  'early_terminate': {\n      'type': 'hyperband', 'min_iter': 2, 'max_iter': 8, 's': 2\n  },\n  'parameters': {\n    'embedding_dim': {'values': [16, 32, 64, 256]},\n    'hidden_size':    {'values': [16, 32, 64, 256]},\n    'encoder_layers': {'values': [1, 2, 3]},\n    'decoder_layers': {'values': [1, 2, 3]},\n    'cell_type':      {'values': ['RNN','GRU','LSTM']},\n    'dropout_p':      {'values': [0.2, 0.3, 0.4]},\n    'beam_width':     {'values': [1, 3, 5]},\n    'teacher_forcing_ratio' : {'values': [0.0, 0.3, 0.5,0.7,1.0]}\n  }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.351672Z","iopub.execute_input":"2025-05-20T19:58:34.351967Z","iopub.status.idle":"2025-05-20T19:58:34.367654Z","shell.execute_reply.started":"2025-05-20T19:58:34.351946Z","shell.execute_reply":"2025-05-20T19:58:34.367016Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train():\n    with wandb.init():\n        cfg = wandb.config\n        run_name = (\n            f\"emb{cfg.embedding_dim}_hid{cfg.hidden_size}\"\n            f\"_enc{cfg.encoder_layers}_dec{cfg.decoder_layers}\"\n            f\"_{cfg.cell_type.lower()}_do{int(cfg.dropout_p*100)}\"\n            f\"_beam{cfg.beam_width}_tf{int(cfg.teacher_forcing_ratio*100)}\"\n        )\n        wandb.run.name = run_name  # Update name after init\n\n        # Load data + build vocab\n        train_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n            BASE_DIR, batch_size=64, build_vocab=True\n        )\n        IDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}\n\n        # Initialize model\n        model = Seq2Seq(\n            input_size=src_size,\n            output_size=tgt_size,\n            embedding_dim=cfg.embedding_dim,\n            hidden_size=cfg.hidden_size,\n            encoder_layers=cfg.encoder_layers,\n            decoder_layers=cfg.decoder_layers,\n            cell_type=cfg.cell_type,\n            dropout_p=cfg.dropout_p,\n            bidirectional_encoder=False\n        ).to(DEVICE)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        criterion = nn.NLLLoss(ignore_index=pad_idx)\n        stopper = EarlyStopper(patience=5)\n        best_val_acc = 0.0\n\n        for epoch in range(1, 11):\n            model.train()\n            total_loss = 0.0\n            for src, tgt in tqdm(train_loader, desc=f\"[Epoch {epoch}] Training\", leave=False):\n                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                optimizer.zero_grad()\n                out = model(src, tgt, teacher_forcing_ratio=cfg.teacher_forcing_ratio)\n                loss = criterion(out.view(-1, tgt_size), tgt.view(-1))\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            # Validation\n            model.eval()\n            correct_seqs, total_seqs = 0, 0\n            with torch.no_grad():\n                for src, tgt in tqdm(val_loader, desc=f\"[Epoch {epoch}] Validation\", leave=False):\n                    src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                    out = model(src, tgt, teacher_forcing_ratio=0.0)\n                    preds = out.argmax(dim=2)\n                    for pred_seq, true_seq in zip(preds, tgt):\n                        pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                        true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                        if torch.equal(pred_tokens, true_tokens):\n                            correct_seqs += 1\n                        total_seqs += 1\n\n            val_acc = correct_seqs / total_seqs\n            wandb.log({'epoch': epoch, 'val_accuracy': val_acc, 'train_loss': total_loss})\n\n            print(f\"[Epoch {epoch}] Train Loss: {total_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n            elif stopper.should_stop(val_acc):\n                print(\"Early stopping triggered.\")\n                break\n\n        # Final test evaluation\n        model.eval()\n        correct_seqs, total_seqs = 0, 0\n        with torch.no_grad():\n            for src, tgt in tqdm(test_loader, desc=\"Final Test Eval\", leave=False):\n                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n                out = model(src, tgt, teacher_forcing_ratio=0.0)\n                preds = out.argmax(dim=2)\n                for pred_seq, true_seq in zip(preds, tgt):\n                    pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                    true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                    if torch.equal(pred_tokens, true_tokens):\n                        correct_seqs += 1\n                    total_seqs += 1\n\n        test_acc = correct_seqs / total_seqs\n        print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n        wandb.log({'final_test_accuracy': test_acc})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T19:58:34.368509Z","iopub.execute_input":"2025-05-20T19:58:34.368721Z","iopub.status.idle":"2025-05-20T19:58:34.387139Z","shell.execute_reply.started":"2025-05-20T19:58:34.368704Z","shell.execute_reply":"2025-05-20T19:58:34.386516Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project='da6401_assignment3')\nwandb.agent(sweep_id, function=train, count=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:56:42.515818Z","iopub.execute_input":"2025-05-18T17:56:42.516119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Best Model ","metadata":{}},{"cell_type":"code","source":"CHAR2IDX_SRC = {\n    \"<pad>\": 0,\n    \"<sos>\": 1,\n    \"<eos>\": 2,\n    \"<unk>\": 3,\n    **{c: i + 4 for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n}\nIDX2CHAR_SRC = {i: c for c, i in CHAR2IDX_SRC.items()}\n\n# Load data, build vocabs, and create reverse target-char map\ntrain_loader, val_loader, test_loader, src_size, tgt_size, pad_idx, src_vocab, tgt_vocab = get_dataloaders(\n    BASE_DIR, batch_size=64, build_vocab=True\n)\n\nIDX2CHAR_TGT = {idx: ch for ch, idx in tgt_vocab.items()}  # Map decoder indices back to Telugu chars\n\n# Model, optimizer, loss, early stopping\n\n# parameters of best model\nbest_model = Seq2Seq(\n    input_size=src_size,\n    output_size=tgt_size,\n    embedding_dim=64,\n    hidden_size=256,\n    encoder_layers=3,\n    decoder_layers=3,\n    cell_type='LSTM',  # or 'GRU' or 'RNN'\n    dropout_p=0.2,\n    bidirectional_encoder=False\n).to(DEVICE)\n\noptimizer = torch.optim.Adam(best_model.parameters(), lr=1e-3)\ncriterion = nn.NLLLoss(ignore_index=pad_idx)\nstopper = EarlyStopper(patience=5)\nbest_val_acc = 0.0\n\n# Training Loop\n\nfor epoch in range(1, 11):\n    best_model.train()\n    total_loss = 0.0\n    for src, tgt in tqdm(train_loader, desc=f\"[Epoch {epoch}] Training\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        optimizer.zero_grad()\n        out = best_model(src, tgt, teacher_forcing_ratio=1.0)\n        loss = criterion(out.view(-1, tgt_size), tgt.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    # Validation: sequence-level accuracy\n    best_model.eval()\n    correct_seqs, total_seqs = 0, 0\n    with torch.no_grad():\n        for src, tgt in tqdm(val_loader, desc=f\"[Epoch {epoch}] Validation\", leave=False):\n            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n            out = best_model(src, tgt, teacher_forcing_ratio=0.0)\n            preds = out.argmax(dim=2)\n            for pred_seq, true_seq in zip(preds, tgt):\n                # Remove <sos> and padding tokens for comparison\n                pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n                true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n                if torch.equal(pred_tokens, true_tokens):\n                    correct_seqs += 1\n                total_seqs += 1\n\n    val_acc = correct_seqs / total_seqs\n    print(f\"[Epoch {epoch}] Loss: {total_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # Optionally save model checkpoint here\n    elif stopper.should_stop(val_acc):\n        print(\"Early stopping triggered.\")\n        break\n\n# Final Test Evaluation\nbest_model.eval()\ncorrect_seqs, total_seqs = 0, 0\nall_preds, all_trues = [], []\n\nwith torch.no_grad():\n    for src, tgt in tqdm(test_loader, desc=\"Final Test Eval\", leave=False):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out = best_model(src, tgt, teacher_forcing_ratio=0.0)\n        preds = out.argmax(dim=2)\n        for pred_seq, true_seq in zip(preds, tgt):\n            pred_tokens = pred_seq[1:][true_seq[1:] != pad_idx]\n            true_tokens = true_seq[1:][true_seq[1:] != pad_idx]\n            if torch.equal(pred_tokens, true_tokens):\n                correct_seqs += 1\n            total_seqs += 1\n\n            all_preds.append(pred_tokens)\n            all_trues.append(true_tokens)\n\ntest_acc = correct_seqs / total_seqs\nprint(f\"\\n Final Test Accuracy (Exact Word match): {test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"romanized_test_words = []\nwith open(os.path.join(BASE_DIR, 'te.translit.sampled.test.tsv'), \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        telugu, roman, _ = line.strip().split()\n        romanized_test_words.append(roman)\n\nbest_model.eval()\nsamples = []\n\nwith torch.no_grad():\n    for i, (src, tgt) in enumerate(test_loader):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        out = best_model(src, tgt, teacher_forcing_ratio=0.0)\n        preds = out.argmax(dim=2)\n\n        for b in range(src.shape[0]):\n            pred = ''.join(\n                IDX2CHAR_TGT[idx.item()] \n                for idx in preds[b][1:] \n                if idx.item() not in (pad_idx,)\n            )\n            true = ''.join(\n                IDX2CHAR_TGT[idx.item()] \n                for idx in tgt[b][1:] \n                if idx.item() not in (pad_idx,)\n            )\n\n            romanized = romanized_test_words[i * src.shape[0] + b]\n\n            samples.append({\n                'Input': romanized,\n                'True Telugu'             : true,\n                'Predicted Telugu'        : pred\n            })\n\n# Sample and show table\nsubset = random.sample(samples, min(10, len(samples)))\ndf = pd.DataFrame(subset)\nprint(df.to_markdown(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:05:49.779336Z","iopub.execute_input":"2025-05-20T20:05:49.780324Z","iopub.status.idle":"2025-05-20T20:06:19.328693Z","shell.execute_reply.started":"2025-05-20T20:05:49.780285Z","shell.execute_reply":"2025-05-20T20:06:19.328073Z"}},"outputs":[{"name":"stdout","text":"| Input            | True Telugu   | Predicted Telugu   |\n|:-----------------|:--------------|:-------------------|\n| sikshnhaa        | శిక్షణా<eos>     | సిక్షణా<eos>          |\n| rahasyamuga      | రహస్యముగా<eos>   | రాహస్యముగా             |\n| kuuragaayalu     | కూరగాయలు<eos>    | కూరగాయలు<eos>         |\n| varthapathrikalu | వార్తాపత్రికలు<eos> | వర్తపాత్రికలు<eos>ు      |\n| mandalamulo      | మండలములో<eos>    | మండలములో<eos>         |\n| akramaalu        | అక్రమాలు<eos>    | ఆక్రమాలు<eos>         |\n| vikalangula      | వికలాంగుల<eos>    | వికలంగుల<eos>ు         |\n| daariloone       | దారిలోనే<eos>     | దారిలోనే<eos>          |\n| bhawanaalalo     | భవనాలలో<eos>    | భావనాలలో              |\n| gramddhaalayam   | గ్రంథాలయం<eos>    | గ్రంథాలయాం              |\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def highlight_pred(row):\n    \"\"\"\n    Returns a list of CSS styles, one per column, \n    coloring the 'Predicted Telugu' cell green if correct else red.\n    \"\"\"\n    styles = [''] * len(row)\n    # Find the index of the Predicted column\n    pred_idx = list(row.index).index('Predicted Telugu')\n    if row['Predicted Telugu'] == row['True Telugu']:\n        styles[pred_idx] = 'background-color: #c8e6c9; font-weight: bold;'  # light green\n    else:\n        styles[pred_idx] = 'background-color: #f8d7da; font-weight: bold;'  # light red\n    return styles\n\n# Apply to a random sample of 10 rows\nsubset = df.sample(n=min(10, len(df))).reset_index(drop=True)\n\nstyled = (\n    subset.style\n          .apply(highlight_pred, axis=1)\n          .set_table_styles([\n              # Center all text\n              {'selector': 'td, th',\n               'props': [('text-align', 'center'), ('padding', '6px')]},\n              # Header style\n              {'selector': 'th',\n               'props': [('background-color', '#4F81BD'),\n                         ('color', 'white'),\n                         ('font-weight', 'bold'),\n                         ('padding', '8px')]}\n          ])\n          .set_caption(\" Sample Transliteration Predictions (Green = Correct, Red = Wrong) \")\n)\n\n# Display in a Jupyter/HTML context\ndisplay(styled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:06:19.421008Z","iopub.execute_input":"2025-05-20T20:06:19.421509Z","iopub.status.idle":"2025-05-20T20:06:19.431856Z","shell.execute_reply.started":"2025-05-20T20:06:19.421490Z","shell.execute_reply":"2025-05-20T20:06:19.431164Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7b92b2fb17d0>","text/html":"<style type=\"text/css\">\n#T_5a080 td {\n  text-align: center;\n  padding: 6px;\n}\n#T_5a080  th {\n  text-align: center;\n  padding: 6px;\n}\n#T_5a080 th {\n  background-color: #4F81BD;\n  color: white;\n  font-weight: bold;\n  padding: 8px;\n}\n#T_5a080_row0_col2, #T_5a080_row1_col2, #T_5a080_row2_col2, #T_5a080_row3_col2, #T_5a080_row4_col2, #T_5a080_row5_col2, #T_5a080_row9_col2 {\n  background-color: #f8d7da;\n  font-weight: bold;\n}\n#T_5a080_row6_col2, #T_5a080_row7_col2, #T_5a080_row8_col2 {\n  background-color: #c8e6c9;\n  font-weight: bold;\n}\n</style>\n<table id=\"T_5a080\">\n  <caption> Sample Transliteration Predictions (Green = Correct, Red = Wrong) </caption>\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_5a080_level0_col0\" class=\"col_heading level0 col0\" >Input</th>\n      <th id=\"T_5a080_level0_col1\" class=\"col_heading level0 col1\" >True Telugu</th>\n      <th id=\"T_5a080_level0_col2\" class=\"col_heading level0 col2\" >Predicted Telugu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_5a080_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_5a080_row0_col0\" class=\"data row0 col0\" >bhawanaalalo</td>\n      <td id=\"T_5a080_row0_col1\" class=\"data row0 col1\" >భవనాలలో<eos></td>\n      <td id=\"T_5a080_row0_col2\" class=\"data row0 col2\" >భావనాలలో</td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_5a080_row1_col0\" class=\"data row1 col0\" >akramaalu</td>\n      <td id=\"T_5a080_row1_col1\" class=\"data row1 col1\" >అక్రమాలు<eos></td>\n      <td id=\"T_5a080_row1_col2\" class=\"data row1 col2\" >ఆక్రమాలు<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_5a080_row2_col0\" class=\"data row2 col0\" >gramddhaalayam</td>\n      <td id=\"T_5a080_row2_col1\" class=\"data row2 col1\" >గ్రంథాలయం<eos></td>\n      <td id=\"T_5a080_row2_col2\" class=\"data row2 col2\" >గ్రంథాలయాం</td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_5a080_row3_col0\" class=\"data row3 col0\" >sikshnhaa</td>\n      <td id=\"T_5a080_row3_col1\" class=\"data row3 col1\" >శిక్షణా<eos></td>\n      <td id=\"T_5a080_row3_col2\" class=\"data row3 col2\" >సిక్షణా<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_5a080_row4_col0\" class=\"data row4 col0\" >vikalangula</td>\n      <td id=\"T_5a080_row4_col1\" class=\"data row4 col1\" >వికలాంగుల<eos></td>\n      <td id=\"T_5a080_row4_col2\" class=\"data row4 col2\" >వికలంగుల<eos>ు</td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_5a080_row5_col0\" class=\"data row5 col0\" >varthapathrikalu</td>\n      <td id=\"T_5a080_row5_col1\" class=\"data row5 col1\" >వార్తాపత్రికలు<eos></td>\n      <td id=\"T_5a080_row5_col2\" class=\"data row5 col2\" >వర్తపాత్రికలు<eos>ు</td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_5a080_row6_col0\" class=\"data row6 col0\" >daariloone</td>\n      <td id=\"T_5a080_row6_col1\" class=\"data row6 col1\" >దారిలోనే<eos></td>\n      <td id=\"T_5a080_row6_col2\" class=\"data row6 col2\" >దారిలోనే<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_5a080_row7_col0\" class=\"data row7 col0\" >mandalamulo</td>\n      <td id=\"T_5a080_row7_col1\" class=\"data row7 col1\" >మండలములో<eos></td>\n      <td id=\"T_5a080_row7_col2\" class=\"data row7 col2\" >మండలములో<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_5a080_row8_col0\" class=\"data row8 col0\" >kuuragaayalu</td>\n      <td id=\"T_5a080_row8_col1\" class=\"data row8 col1\" >కూరగాయలు<eos></td>\n      <td id=\"T_5a080_row8_col2\" class=\"data row8 col2\" >కూరగాయలు<eos></td>\n    </tr>\n    <tr>\n      <th id=\"T_5a080_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_5a080_row9_col0\" class=\"data row9 col0\" >rahasyamuga</td>\n      <td id=\"T_5a080_row9_col1\" class=\"data row9 col1\" >రహస్యముగా<eos></td>\n      <td id=\"T_5a080_row9_col2\" class=\"data row9 col2\" >రాహస్యముగా</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Ensure the output folder exists\nos.makedirs(\"predictions_vanilla\", exist_ok=True)\n\ndf1 = pd.DataFrame(samples)\ndf1.to_csv(\"predictions_vanilla/predictions.csv\", index=False, encoding=\"utf-8-sig\")\n\nprint(\" Saved all predictions to predictions_vanilla/predictions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:06:19.432536Z","iopub.execute_input":"2025-05-20T20:06:19.432850Z","iopub.status.idle":"2025-05-20T20:06:19.471137Z","shell.execute_reply.started":"2025-05-20T20:06:19.432828Z","shell.execute_reply":"2025-05-20T20:06:19.470453Z"}},"outputs":[{"name":"stdout","text":" Saved all predictions to predictions_vanilla/predictions.csv\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}